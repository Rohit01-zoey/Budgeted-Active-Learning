In the knowledge distillation process, the labeler or teacher model is typically large and complex. Consequently, the costs (be it hardware resources or inference frequency) linked with using the teacher model to label data increase proportionally with the size of the unlabeled dataset. Our primary goal is to create an algorithmic framework designed to efficiently sample from this vast pool of unlabeled data, specifically targeting the most "informative" instances. This approach aims to significantly reduce the costs associated with querying the teacher model.